{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoders\n",
    "\n",
    "We will implement a Variational AutoEncoder and run it on face data.\n",
    "\n",
    "Once you are satisfied with your implementation, send hw2.ipynb to **vjojic+hw2+comp790@cs.unc.edu**.\n",
    "\n",
    "Save your python notebook frequently.\n",
    "\n",
    "All you have to do is replace ```...``` with appropriate code.\n",
    "\n",
    "The deadline for this homework assignment is 4/4/2017 23:59PM EST.\n",
    "$ %These are LaTeX definitions used below to save space\n",
    "\\newcommand{\\zz}{\\mathbf{z}} \\newcommand{\\xx}{\\mathbf{x}} \n",
    "\\newcommand{\\hh}{\\mathbf{h}} \\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\dec}{\\textrm{dec}} \\newcommand{\\enc}{\\textrm{enc}} \n",
    "\\newcommand{\\rec}{\\textrm{rec}}\n",
    "\\newcommand{\\relu}{\\textrm{ReLU}} \\newcommand{\\mmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\ssigma}{\\boldsymbol{\\sigma}} \\newcommand{\\eepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\KL}{\\textrm{KL}}\n",
    "$\n",
    "\n",
    "## Load data\n",
    "First we will look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline \n",
    "faces = np.load('freyfaces.npy',encoding='latin1')\n",
    "print(\"Shape of data (samples x dimensions):\",faces.shape)\n",
    "print(\"Each row of data matrix is an image consisting of 20*28=560 pixels.\")\n",
    "print(\"Some images from the dataset\")\n",
    "def show_examples(x,square=True):\n",
    "    N = x.shape[0]\n",
    "    if square:\n",
    "        d = int(np.ceil(np.sqrt(N)))\n",
    "        d1 = int(np.ceil(N/d))\n",
    "    else:\n",
    "        d = N\n",
    "        d1 = 1\n",
    "        \n",
    "    im = np.zeros([d1*28,d*20])\n",
    "    for i in range(d1):\n",
    "        for j in range(d):\n",
    "            c = i*d + j\n",
    "            if c<N:\n",
    "                im[i*28:(i+1)*28,j*20:(j+1)*20] = x[c,:].reshape([28,20])\n",
    "    pyplot.figure(figsize=[d,d1])\n",
    "    pyplot.imshow(im,interpolation=None,cmap='Greys')\n",
    "    \n",
    "show_examples(faces[0:1500:50,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano\n",
    "Next we are going to play with Theano. Read the code below and Tianxiang's tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "theano.config.optimization='fastrun'\n",
    "theano.config.exception_verbosity='high'\n",
    "\n",
    "def make_shared(name,value):      \n",
    "    if value is np.ndarray:\n",
    "        value = value.astype('float32')\n",
    "    else:\n",
    "        value = np.float32(value)\n",
    "    return theano.shared(name=name,value=value.astype('float32'))\n",
    "    \n",
    "\n",
    "# declare data matrix\n",
    "X = T.dmatrix('X').astype('float32')\n",
    "y = T.dvector('y').astype('float32')\n",
    "# declare shared variables -- parameters -- W and b\n",
    "w = make_shared(name='w',value=np.zeros(5))\n",
    "b = make_shared(name='b',value=0)\n",
    "prediction = T.dot(X,w) + b\n",
    "\n",
    "# construct cost. \n",
    "# Note that that cost encodes the symbolic expression\n",
    "# there is no actual computation of the cost yet.\n",
    "cost = 0.5*T.mean((prediction - y)**2.0) \n",
    "\n",
    "# Use the symbolic form of the cost and compute gradients\n",
    "# with respect to parameters\n",
    "gw,gb = T.grad(cost,[w,b])\n",
    "\n",
    "# Compile a theano function which takes as input\n",
    "# values to plug-in in place of symbolic variables X and y,\n",
    "# outputs evaluated cost, and performs updates. \n",
    "# Note that updates are specified as pairs:\n",
    "# (<variable name>,<expression used to update the variable>).\n",
    "# You should recognize gradient descent update here.\n",
    "train = theano.function(inputs=[X,y],\n",
    "                        outputs=cost,\n",
    "                        updates=((w, w - 0.05 * gw), \n",
    "                                 (b, b - 0.05 * gb)))\n",
    "\n",
    "# synthesize some test data\n",
    "# Note that only here we talk about dimensionality \n",
    "# of the data.\n",
    "N = 100\n",
    "d = 5\n",
    "np.random.seed(1)\n",
    "data_X = np.random.randn(N,d).astype('float32')\n",
    "true_w = np.zeros(d).astype('float32')\n",
    "true_w[0:1] = 1.0\n",
    "true_b = 5.0\n",
    "true_y = np.dot(data_X,true_w) + true_b\n",
    "data_y = true_y + 0.1*np.random.randn(N).astype('float32')\n",
    "\n",
    "# starting state for w and b\n",
    "print(\"Starting w: \",w.get_value(),\"\\nStarting b:\",b.get_value(),'\\n')    \n",
    "\n",
    "costs = []\n",
    "for i in range(100):\n",
    "    # each call to train updates parameters\n",
    "    # w and b\n",
    "    current_cost = train(data_X,data_y)\n",
    "    if i%10 == 0:\n",
    "        costs.append(current_cost)\n",
    "        print('cost for iter',i,' ',current_cost)\n",
    "\n",
    "print(\"Final w: \",w.get_value(),\"\\nFinal b:\",b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "**Problem 1. (1pt) ** Next we are going to change the cost and add a weight decay (ridge penalty) term.\n",
    "This term is $$\\sum_i w_i^2.$$\n",
    "Introduce new variable ```cost2``` that takes ```cost``` and adds weight decay term to it. Hint: T.sum(z) creates a symbolic expression for the sum of entries in z.\n",
    "\n",
    "Note, that you have to compute gradients of the new cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cost2 = cost + 0.1*T.sum(w**2)\n",
    "\n",
    "\n",
    "gw2,gb2 = T.grad(cost2,[w,b])\n",
    "train2 = theano.function(inputs=[X,y],\n",
    "                        outputs=cost,\n",
    "                        updates=((w, w - 0.05 * gw2), \n",
    "                                 (b, b - 0.05 * gb2)))\n",
    "\n",
    "# We reset parameters to zeros. \n",
    "b.set_value(0.0)\n",
    "w.set_value(np.zeros(5,dtype='float32'))\n",
    "\n",
    "costs = []\n",
    "for i in range(100):\n",
    "    train2(data_X,data_y)\n",
    "\n",
    "print(\"w: \",w.get_value(),\"\\nb:\",b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Problem 2. (1pt)** Implement reparameterization trick. Recall that reparametrization trick takes mean and variance and computes new samples\n",
    "$$\n",
    "z_{i,j} = \\mu_i + \\sigma_i*\\epsilon_{i,j},\n",
    "$$\n",
    "where $$\\epsilon_i \\sim \\mathcal{N}(0,I_p),$$ and  $\\mu$ and $\\sigma$ are computed by a neural network.\n",
    "\n",
    "In our code, we use the theano to capture random number generation. We cannot rely on using numpy, because that would hardcode the random numbers. Rather these random numbers need to be generated repeatedly.\n",
    "\n",
    "Implement a function that computes $\\zz$ from $\\eepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampler(mu, sigma, seed=1):                         \n",
    "    srng = T.shared_randomstreams.RandomStreams(seed=seed)\n",
    "    eps = srng.normal(mu.shape)\n",
    "\n",
    "\n",
    "    z = mu +  sigma* eps\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3. (1pt)** Specify a neural network that computes function $g$ from the notes -- encoder. This network takes as input data vector $\\mathbf{x}$  and produces mean and standard deviations for each entry in code $\\mathbf{z}$. We will use a simple one layer network with ReLU activation.\n",
    "\n",
    "Note: standard deviations and variances are positive quantities. Hence we will have the neural network compute *logarithm* of variance, which can be either positive or negative.\n",
    "\n",
    "Specification of the network is given here\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\hh_{\\enc} &=& \\relu(\\xx^TW_{\\enc} + \\bb_{enc})\\\\\n",
    "\\mmu_{\\enc} &=& h_{\\enc}^T W_{\\enc\\mmu} + b_{\\enc\\mmu} \\\\\n",
    "\\log \\ssigma_{\\enc}^2  &=& h_{\\enc}^T W_{\\enc\\ssigma} + b_{\\enc\\ssigma}.\n",
    "\\end{array}\n",
    "$$\n",
    "Write code to create shared variables $W_{\\enc} ,\\bb_{\\enc},W_{\\enc\\mmu}, b_{\\enc\\mmu},W_{\\enc\\ssigma}, b_{\\enc\\ssigma}$.\n",
    "\n",
    "Using these shared variables in constructing symbolic expressions for computing $\\hh_{\\enc},\\mmu_{\\enc},\\ssigma_{\\enc}$.\n",
    "\n",
    "Below, we use $\\xx$ for feature vector, $d$ for number of features, $nh$ as number of units in the encoder (size of $\\hh_{\\enc}$), $n_z$ as the length of code ($\\zz$). In figuring out the sizes for different parameter vectors and matrices, use the specification above and dimensions of the objects being multiplied. For example: computation of $\\hh_{\\enc}$ is of size $nh$ and $\\xx$ is of size $d$ hence $W_{\\enc}$ is of size $d \\times n_h$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return T.switch(x<0, 0, x)\n",
    "\n",
    "x = T.dmatrix('x').astype('float32')\n",
    "d = faces.shape[1]\n",
    "nh = 256\n",
    "nz = 4\n",
    "\n",
    "W_enc = make_shared(name='W_enc',value=0.01*np.random.randn(d,nh))\n",
    "b_enc = make_shared(name='b_enc',value=np.zeros(nh))\n",
    "\n",
    "\n",
    "W_encmu = make_shared(name='W_encmu',value=0.01*np.random.randn(nh,nz))\n",
    "b_encmu = make_shared(name='b_encmu',value=np.zeros(nz))\n",
    "W_encsigma = make_shared(name='W_encsigma',value=0.01*np.random.randn(nh,nz))\n",
    "b_encsigma = make_shared(name='b_encsigma',value=np.zeros(nz))\n",
    "\n",
    "h_enc = relu(T.dot(x, W_enc) + b_enc)\n",
    "mu_enc = T.dot(h_enc,W_encmu) + b_encmu\n",
    "log_sigma2_enc = T.dot(h_enc, W_encsigma) + b_encsigma\n",
    "\n",
    "\n",
    "encoder_params = [W_enc,b_enc,W_encmu,b_encmu,W_encsigma,b_encsigma]\n",
    "sigma_enc = T.exp(0.5*log_sigma2_enc)\n",
    "\n",
    "z = sampler(mu=mu_enc,sigma=sigma_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4. (1pt)** Specify a neural network that computes function $f$ from the notes -- decoder. This network takes as input code vector $\\zz$ and produces mean and standard deviations for each entry in the reconstruction of the data $\\xx$.\n",
    "\n",
    "Thes set-up is symmetric to the encoder\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\hh_{\\dec} &=& \\relu(\\zz^TW_{\\dec} + \\bb_{dec})\\\\\n",
    "\\mmu_{\\dec} &=& h_{\\dec}^T W_{\\dec\\mmu} + b_{\\dec\\mmu} \\\\\n",
    "\\log \\ssigma_{\\dec}^2  &=& h_{\\dec}^T W_{\\dec\\ssigma} + b_{\\dec\\ssigma}.\n",
    "\\end{array}\n",
    "$$\n",
    "Write code to create shared variables $W_{\\dec} ,\\bb_{\\dec},W_{\\dec\\mmu}, b_{\\dec\\mmu},W_{\\dec\\ssigma}, b_{\\dec\\ssigma}$.\n",
    "\n",
    "Using these shared variables in constructing symbolic expressions for computing $\\hh_{\\dec},\\mmu_{\\dec},\\ssigma_{\\dec}$.\n",
    "\n",
    "Note that $\\mu_{\\dec}$ and $\\sigma_{\\dec}$ are of the same length as feature vector $\\xx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_dec = make_shared(name='W_dec',value=0.01*np.random.randn(nz,nh))\n",
    "b_dec = make_shared(name='b_dec',value=np.zeros(nh))\n",
    "\n",
    "W_decmu = make_shared(name='W_decmu',value=0.01*np.random.randn(nh,d))\n",
    "b_decmu = make_shared(name='b_decmu',value=np.zeros(d))\n",
    "W_decsigma = make_shared(name='W_decsigma',value=0.01*np.random.randn(nh,d))\n",
    "b_decsigma = make_shared(name='b_decsigma',value=np.zeros(d))\n",
    "\n",
    "h_dec = relu(T.dot(z, W_dec) + b_dec)\n",
    "mu_dec = T.dot(h_dec, W_decmu) + b_decmu\n",
    "log_sigma2_dec = T.dot(h_dec, W_decsigma) + b_decsigma\n",
    "\n",
    "\n",
    "decoder_params = [W_dec,b_dec,W_decmu,b_decmu,W_decsigma,b_decsigma]\n",
    "sigma_dec = T.exp(0.5*log_sigma2_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Instead of stochastic gradient descent we will use an ADAM implementation.You do not need to change anything in this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an adam implementation from https://github.com/skaae/\n",
    "def adam(loss, all_params, learning_rate=0.0002, beta1=0.1, beta2=0.001,\n",
    "         epsilon=1e-8, gamma=1-1e-7):   \n",
    "    \"\"\"\n",
    "    ADAM update rules\n",
    "    Default values are taken from [Kingma2014]\n",
    "\n",
    "    References:\n",
    "    [Kingma2014] Kingma, Diederik, and Jimmy Ba.\n",
    "    \"Adam: A Method for Stochastic Optimization.\"\n",
    "    arXiv preprint arXiv:1412.6980 (2014).\n",
    "    http://arxiv.org/pdf/1412.6980v4.pdf\n",
    "\n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    all_grads = theano.grad(loss, all_params)\n",
    "\n",
    "    i = theano.shared(np.float32(1))  \n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - (1. - beta1)**i_t\n",
    "    fix2 = 1. - (1. - beta2)**i_t\n",
    "    beta1_t = 1-(1-beta1)*gamma**(i_t-1)  \n",
    "    learning_rate_t = learning_rate * (T.sqrt(fix2) / fix1)\n",
    "\n",
    "    for param_i, g in zip(all_params, all_grads):\n",
    "        m = theano.shared(\n",
    "            np.zeros(param_i.get_value().shape, dtype='float32'))\n",
    "        v = theano.shared(\n",
    "            np.zeros(param_i.get_value().shape, dtype='float32'))\n",
    "\n",
    "        m_t = (beta1_t * g) + ((1. - beta1_t) * m) \n",
    "        v_t = (beta2 * g**2) + ((1. - beta2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + epsilon)\n",
    "        param_i_t = param_i - (learning_rate_t * g_t)\n",
    "\n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((param_i, param_i_t) )\n",
    "    updates.append((i, i_t))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5. (1pt) ** Use theano to implement cost computation. Specifically, you will need to change lines that compute $\\log p(x|z)$ denoted by ```logpxz```,$\\KL(q(\\zz|\\xx),p(\\zz))$ denoted by KLD\n",
    "$$\n",
    "\\log p(\\xx|\\zz) = \\sum_{j=1}^p \\left[ -\\frac{1}{2}\\log \\left\\{2\\pi\\sigma^2_{\\dec,j}\\right\\} - \\frac{1}{2\\sigma^2_{\\dec,j}} (x_j - \\mu_{\\dec,j})^2 \\right]\n",
    "$$\n",
    "$$\n",
    "\\KL(q(\\zz|\\xx),p(\\zz)) = -\\frac{1}{2}\\sum_{j=1}^p \\left[1 + \\log \\sigma_{\\enc,j} - \\mu_{\\enc,j}^2 - \\sigma_{\\enc,j}^2\\right]\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^N\\log p(\\xx^i) \\geq \\frac{1}{N}\\sum_{i=1}^N \\left[ -\\KL(q(\\zz^i|\\xx^i),p(\\zz^i)) + \\frac{1}{L}\\sum_{l=1}^L \\log p(\\xx^{i,l}|\\zz^{i,l}) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_params = encoder_params + decoder_params\n",
    "\n",
    "logpxz = (-(0.5 * np.log(2 * np.pi) + 0.5 * log_sigma2_dec) -\n",
    "            0.5 * ((x - mu_dec)**2 / T.exp(log_sigma2_dec))).sum(axis=1)\n",
    "KLD = -0.5 * T.sum(1 +  np.log(sigma_enc) - mu_enc**2 - T.exp(log_sigma2_enc), axis=1)\n",
    "\n",
    "logpx = T.mean(logpxz - KLD) \n",
    "\n",
    "\n",
    "\n",
    "x_train = faces[0:1500,:].astype('float32')\n",
    "x_valid = faces[1500:,:].astype('float32')\n",
    "\n",
    "batch_size = 100\n",
    "batch_count = x_train.shape[0]//batch_size\n",
    "batches = np.arange(batch_count)\n",
    "\n",
    "updates = adam(-logpx,all_params)\n",
    "likelihood = theano.function([x], logpx)\n",
    "encode = theano.function([x], z)\n",
    "decode = theano.function([z], mu_dec)\n",
    "reconstruct = theano.function([x],mu_dec)\n",
    "train = theano.function(inputs=[x],\n",
    "                        outputs=logpx,\n",
    "                        updates=updates)\n",
    "\n",
    "np.random.seed(1)\n",
    "for i in range(200):        \n",
    "    np.random.shuffle(batches)\n",
    "    for batch in batches:\n",
    "        minibatch = x_train[batch*batch_size:(batch+1)*batch_size, :]\n",
    "        train(minibatch)    \n",
    "    if i % 50 == 0:        \n",
    "        print(\"iter\",i,\"current likelihood\",likelihood(x_valid))\n",
    "                                   \n",
    "show_examples(reconstruct(x_train[1:1500:30,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "starting_face =  np.asmatrix(x_train[800,:])\n",
    "end_face = np.asmatrix(x_train[1,:])\n",
    "\n",
    "# Make changes to compute the start_code\n",
    "# Note that we already have functions encode and decode.\n",
    "# Use them\n",
    "\n",
    "\n",
    "start_code = encode(starting_face)\n",
    "end_code = encode(end_face)\n",
    "d = (end_code - start_code)/20\n",
    "trace = np.zeros([21,560])\n",
    "for i in np.arange(21):\n",
    "    code = start + d*i\n",
    "    trace[i,:] = decode(code)\n",
    "\n",
    "\n",
    "show_examples(trace,square=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}